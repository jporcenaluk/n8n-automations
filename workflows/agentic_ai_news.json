{
  "name": "weekly_agentic_ai_news",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "weeks",
              "triggerAtDay": [
                1
              ],
              "triggerAtHour": 6
            }
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.3,
      "position": [
        0,
        0
      ],
      "id": "7b6e61e3-1669-461f-bf5b-0168ab7bf157",
      "name": "Schedule Trigger"
    },
    {
      "parameters": {
        "language": "pythonNative",
        "pythonCode": "import urllib.request\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime, timedelta, timezone\nfrom email.utils import parsedate_to_datetime\nimport re\n\ndef get_feed_urls():\n    \"\"\"Returns list of RSS feed URLs\"\"\"\n    return [\n        {\"url\": \"https://www.latent.space/feed\"},\n        {\"url\": \"https://simonwillison.net/atom/entries/\"},\n        {\"url\": \"https://blog.pragmaticengineer.com/rss/\"},\n        {\"url\": \"https://www.swyx.io/rss.xml\"},\n        {\"url\": \"https://github.blog/changelog/feed/\"},\n        {\"url\": \"https://openai.com/news/rss.xml\"},\n        {\"url\": \"https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_research.xml\"},\n        {\"url\": \"https://www.reddit.com/r/ClaudeAI.rss\"},\n        {\"url\": \"https://www.reddit.com/r/AI_Agents.rss\"},\n        {\"url\": \"https://github.blog/changelog/label/copilot/\"}\n    ]\n\ndef parse_date(date_string):\n    \"\"\"Parse various date formats found in RSS/Atom feeds. Always returns timezone-aware datetime.\"\"\"\n    if not date_string:\n        return None\n\n    try:\n        # Try RFC 2822 format (common in RSS) - returns timezone-aware datetime\n        return parsedate_to_datetime(date_string)\n    except:\n        pass\n\n    try:\n        # Try ISO 8601 format (common in Atom)\n        # Handle formats like: 2024-01-31T12:00:00Z or 2024-01-31T12:00:00+00:00\n        date_string = date_string.strip()\n        if 'T' in date_string:\n            # Check if it has timezone info\n            if date_string.endswith('Z'):\n                date_string = date_string.rstrip('Z')\n                dt = datetime.fromisoformat(date_string)\n                return dt.replace(tzinfo=timezone.utc)\n            elif re.search(r'[+-]\\d{2}:\\d{2}$', date_string):\n                # Has timezone offset, fromisoformat can handle it in Python 3.7+\n                return datetime.fromisoformat(date_string)\n            else:\n                # No timezone, assume UTC\n                dt = datetime.fromisoformat(date_string)\n                return dt.replace(tzinfo=timezone.utc)\n    except:\n        pass\n\n    return None\n\ndef fetch_feed(url, days_back=7):\n    \"\"\"Fetch and parse RSS/Atom feed, returning entries from the past week\"\"\"\n    try:\n        # Set a user agent to avoid being blocked\n        req = urllib.request.Request(\n            url,\n            headers={'User-Agent': 'Mozilla/5.0 (RSS Reader)'}\n        )\n\n        with urllib.request.urlopen(req, timeout=10) as response:\n            content = response.read()\n\n        # Parse XML\n        root = ET.fromstring(content)\n\n        # Determine feed type (RSS vs Atom)\n        is_atom = root.tag.endswith('feed')\n\n        # Calculate cutoff date (7 days ago) - use UTC timezone for comparison\n        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)\n\n        entries = []\n\n        if is_atom:\n            # Parse Atom feed\n            ns = {'atom': 'http://www.w3.org/2005/Atom'}\n            for entry in root.findall('atom:entry', ns):\n                title = entry.find('atom:title', ns)\n                link = entry.find('atom:link', ns)\n                published = entry.find('atom:published', ns)\n                updated = entry.find('atom:updated', ns)\n                summary = entry.find('atom:summary', ns)\n                content = entry.find('atom:content', ns)\n\n                # Get date\n                date_elem = published if published is not None else updated\n                entry_date = None\n                if date_elem is not None:\n                    entry_date = parse_date(date_elem.text)\n\n                # Filter by date\n                if entry_date and entry_date >= cutoff_date:\n                    entry_dict = {\n                        'title': title.text if title is not None else 'No title',\n                        'link': link.get('href') if link is not None else '',\n                        'date': entry_date.strftime('%Y-%m-%d %H:%M') if entry_date else '',\n                        'description': (content.text if content is not None else\n                                      summary.text if summary is not None else '')[:10000]\n                    }\n                    entries.append(entry_dict)\n        else:\n            # Parse RSS feed\n            channel = root.find('channel')\n            if channel is not None:\n                for item in channel.findall('item'):\n                    title = item.find('title')\n                    link = item.find('link')\n                    pub_date = item.find('pubDate')\n                    description = item.find('description')\n\n                    # Get date\n                    entry_date = None\n                    if pub_date is not None:\n                        entry_date = parse_date(pub_date.text)\n\n                    # Filter by date\n                    if entry_date and entry_date >= cutoff_date:\n                        entry_dict = {\n                            'title': title.text if title is not None else 'No title',\n                            'link': link.text if link is not None else '',\n                            'date': entry_date.strftime('%Y-%m-%d %H:%M') if entry_date else '',\n                            'description': (description.text if description is not None else '')[:10000]\n                        }\n                        entries.append(entry_dict)\n\n        return entries\n\n    except Exception as e:\n        return []\n\ndef format_feed_output(feed_url, entries, max_chars=100000):\n    \"\"\"Format feed entries into a string limited to max_chars\"\"\"\n    output = f\"\\n{'='*80}\\n\"\n    output += f\"Feed: {feed_url}\\n\"\n    output += f\"Entries found: {len(entries)}\\n\"\n    output += f\"{'='*80}\\n\\n\"\n\n    for entry in entries:\n        entry_text = f\"Title: {entry['title']}\\n\"\n        entry_text += f\"Date: {entry['date']}\\n\"\n        entry_text += f\"Link: {entry['link']}\\n\"\n\n        if entry['description']:\n            # Clean HTML tags from description\n            desc = re.sub(r'<[^>]+>', '', entry['description'])\n            entry_text += f\"Description: {desc}\\n\"\n\n        entry_text += f\"{'-'*40}\\n\\n\"\n\n        # Check if adding this entry would exceed the limit\n        if len(output) + len(entry_text) > max_chars:\n            output += f\"\\n[Output truncated at {max_chars} characters]\\n\"\n            break\n\n        output += entry_text\n\n    return output[:max_chars]\n\n# Modified main execution for n8n\nfeeds = get_feed_urls()\nall_results = []\n\nfor feed_info in feeds:\n    url = feed_info['url']\n    entries = fetch_feed(url, days_back=7)\n    formatted = format_feed_output(url, entries, max_chars=100000)\n    all_results.append(formatted)\n\n# Combine all results\nfinal_output = \"\\n\\n\".join(all_results)\n\n# Return in n8n format\nreturn [{\"json\": {\"content\": final_output}}]"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        208,
        0
      ],
      "id": "17f5c269-a2d7-403c-b84e-40ee9a4e1da2",
      "name": "Fetch and Parse RSS Feeds"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=I'm looking to get a feel for the 'zeitgeist' this past week from the news regarding Agentic AI. It is important to sift through the news and understand what is pertinant to a modern software engineer who has adopted AI tools such as Claude Code and GitHub Copilot.\n\nKeep in mind: one week's worth of 'zeitgeist' probably is fairly minimal, so don't make sweeping claims like 'AI has moved from copilot to agentic terminal'. I know that already.\nThe output should be:\n\n1. biggest headline of the week (most impactful; something many people are trying or adopting, has made a big difference to software engineers' workflows, or seems most promising to: it has to be very impactful)\n  a) this should be summarised in 500 words or less\n  b) include a link to the relevant story\n2. top 5 stories (aside from the headline)\n  a) each should be summarised in 100 words or less\n  b) there should be a link to each story\n3. final summary\n  a) give a summary for the week's news, indicating the direction things seem to be headed, any 'gotchas' or learnings, just keep me informed.\n\nThis can be considered like the 'briefing' that arrives on a U.S. President's desk, but on a weekly basis and only relating to tools, services, etc. that use AI and are made for software engineers.\n\n## Sources\n\nHere is the parsed RSS feed content from the past 7 days:\n\n{{ $json.content }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 3.1,
      "position": [
        624,
        0
      ],
      "id": "a8c6eae8-8adc-4f06-8443-4c73600dad3c",
      "name": "AI Agent",
      "executeOnce": true,
      "retryOnFail": true,
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "modelName": "models/gemini-3-pro-preview",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        624,
        208
      ],
      "id": "787a45d4-e3e2-4aa5-8d30-c9a4a856258f",
      "name": "Google Gemini Chat Model",
      "executeOnce": true,
      "alwaysOutputData": false,
      "retryOnFail": true,
      "credentials": {
        "googlePalmApi": {
          "id": "9bHk3I8DguD5HKx9",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "sendTo": "jporcenaluk@gmail.com",
        "subject": "=Agentic AI Weekly Report: {{ $now.format('yyyy-MM-dd') }}",
        "message": "=Here is your weekly report:\n\n{{ $json.output }}",
        "options": {}
      },
      "type": "n8n-nodes-base.gmail",
      "typeVersion": 2.2,
      "position": [
        1344,
        0
      ],
      "id": "02759d55-903d-40f2-b617-d2107d807491",
      "name": "Send Email Report",
      "webhookId": "c12fa610-5830-4095-a412-2df4859c07ea",
      "credentials": {
        "gmailOAuth2": {
          "id": "sI2cofHaaSjw9MKK",
          "name": "Gmail account"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.informationExtractor",
      "typeVersion": 1.2,
      "position": [
        976,
        0
      ],
      "id": "2a686765-5303-48f4-be5e-5d58d4bb1be1",
      "name": "Information Extractor"
    }
  ],
  "pinData": {},
  "connections": {
    "Schedule Trigger": {
      "main": [
        [
          {
            "node": "Fetch and Parse RSS Feeds",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch and Parse RSS Feeds": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "AI Agent": {
      "main": [
        [
          {
            "node": "Information Extractor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Information Extractor": {
      "main": [
        [
          {
            "node": "Send Email Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false
  },
  "versionId": "851b482b-ea1b-405f-975d-112a62afb031",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "f9d218eb6010c95afc6aa0c9e60938515fbd37e477d70e7e75d1385cd65ce6d1"
  },
  "id": "5n_OtnLzjm_yLX2nVyA_S",
  "tags": []
}